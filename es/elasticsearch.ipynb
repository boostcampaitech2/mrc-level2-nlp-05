{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de676479-7790-4bbe-8b08-9ab2f36b4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install elasticsearch\n",
    "# !wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
    "# !tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
    "# !chown -R daemon:daemon elasticsearch-7.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5909295d-8f34-487d-b62b-1d96972645f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! /opt/ml/code/elasticsearch-7.9.2/bin/elasticsearch-plugin install analysis-nori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2f0f3-2cb8-46ba-97d8-e6bd5341509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "es_server = Popen(['elasticsearch-7.9.2/bin/elasticsearch'],\n",
    "                   stdout=PIPE, stderr=STDOUT,\n",
    "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                  )\n",
    "# wait until ES has started\n",
    "! sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f793fcb-171d-4b7f-ab84-9d86c0debdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch('localhost:9200')\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc04de1-6afc-49bb-8d5d-bfd2f3fd3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28cfa8a-8281-40f5-b54f-efe56b135e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.get('document')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a34b338-0222-418f-8c99-d29ed2f73fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('/opt/ml/data/wikipedia_documents.json', 'r') as f:\n",
    "    wiki_data = pd.DataFrame(json.load(f)).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a7b4bd-07f2-4cad-90ad-9f5e5d0df396",
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.delete(index=\"document\", ignore=[400, 404])\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "mapping = {\n",
    "                      'settings':{\n",
    "                          'analysis':{\n",
    "                              'analyzer':{\n",
    "                                  'my_analyzer':{\n",
    "                                      \"type\": \"custom\",\n",
    "                                      'tokenizer':'nori_tokenizer',\n",
    "                                      'decompound_mode':'mixed',\n",
    "                                      'stopwords':'_korean_',\n",
    "                                      \"filter\": [\"lowercase\",\n",
    "                                                 \"my_shingle_f\",\n",
    "                                                 \"nori_readingform\",\n",
    "                                                 \"nori_number\"]\n",
    "                                  }\n",
    "                              },\n",
    "                              'filter':{\n",
    "                                  'my_shingle_f':{\n",
    "                                      \"type\": \"shingle\"\n",
    "                                  }\n",
    "                              }\n",
    "                          },\n",
    "                          'similarity':{\n",
    "                              'my_similarity':{\n",
    "                                  'type':'BM25',\n",
    "                              }\n",
    "                          }\n",
    "                      },\n",
    "                      'mappings':{\n",
    "                          'properties':{\n",
    "                              'title':{\n",
    "                                  'type':'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'similarity':'my_similarity'\n",
    "                              },\n",
    "                              'text':{\n",
    "                                  'type':'text',\n",
    "                                  'analyzer':'my_analyzer',\n",
    "                                  'similarity':'my_similarity'\n",
    "                              }\n",
    "                          }\n",
    "                      }\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275dbaf9-06b3-4bee-acfe-539cd02c7e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=\"document\", custom_mapping=mapping)\n",
    "\n",
    "import json\n",
    "with open('/opt/ml/data/wikipedia_documents.json', \"r\") as f:\n",
    "    wiki = json.load(f)\n",
    "contexts = list(dict.fromkeys([v['text'] for v in wiki.values()]))\n",
    "\n",
    "\n",
    "dicts = [\n",
    "    {\n",
    "        'text': context,\n",
    "    } for context in tqdm(contexts)\n",
    "]\n",
    "document_store.write_documents(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69561253-2409-4171-aae3-c72d917b3bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#only retriever\n",
    "from haystack.retriever import ElasticsearchRetriever\n",
    "retriever = ElasticsearchRetriever(document_store)\n",
    "from haystack.pipeline import DocumentSearchPipeline\n",
    "pipe = DocumentSearchPipeline(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a29e06-550d-430a-9fd0-19665d610107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #reader(FARM of Transformer) +retriever\n",
    "# from haystack.reader.farm import FARMReader\n",
    "# from haystack.reader.transformers import TransformersReader\n",
    "# reader = TransformersReader(model_name_or_path=\"/opt/ml/code/models/koelectra_test\",max_seq_len=300, doc_stride=120)\n",
    "\n",
    "# from haystack.pipeline import ExtractiveQAPipeline\n",
    "# pipe = ExtractiveQAPipelinee(reader,retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7940a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "testset=load_from_disk('/opt/ml/data/test_dataset')\n",
    "testset=testset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=testset[0]['question']\n",
    "prediction = pipe.run(query=question, params={\"retriever\": {\"top_k\": 30}})\n",
    "prediction['documents'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24deb90f-7c8e-4507-8119-54c839d4ef26",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #dictionary로 top1 문서내용을 json으로 저장\n",
    "# result={}\n",
    "# #for i in range(10): #len(testset['question'])\n",
    "# for example in testset:\n",
    "#     question=example[\"question\"]\n",
    "#     prediction = pipe.run(query=question, params={\"retriever\": {\"top_k\": 10}})\n",
    "#     #from haystack.utils import print_answers\n",
    "#     if prediction['d'][0]['probability']>0.99:\n",
    "#         result[example[\"id\"]]=prediction['answers'][0]['answer']\n",
    "#     else:\n",
    "#         result[example[\"id\"]]=None\n",
    "\n",
    "# with open('elastic_reader.json', \"w\") as writer:\n",
    "#     writer.write(json.dumps(result, indent=4, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = testset[0]['question']\n",
    "# top_k_docs = pipe.run(question, params={\"retriever\": {\"top_k\": 10}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530d030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #top1 score json으로 저장\n",
    "# save_score={}\n",
    "\n",
    "# for idx, example in enumerate(tqdm(testset, desc=\"elasticsearch: \")):\n",
    "#     # relev_doc_ids = [el for i, el in enumerate(self.ids) if i in doc_indices[idx]]\n",
    "#     question=example[\"question\"]\n",
    "#     top_k_docs = pipe.run(question, params={\"retriever\": {\"top_k\": 10}})\n",
    "\n",
    "#     query = {\n",
    "#         'query':{\n",
    "#             'bool':{\n",
    "#                 'must':[\n",
    "#                           {'match':{'text':question}}\n",
    "#                 ],\n",
    "#                 'should':[\n",
    "#                           {'match':{'text':question}}\n",
    "#                 ]\n",
    "#             }\n",
    "#         }\n",
    "#     }\n",
    "#     doc = es.search(index='document',body=query,size=30)['hits']['hits']\n",
    "\n",
    "    \n",
    "#     save_score[example['id']]=doc[0]['_score']\n",
    "    \n",
    "# with open('top1_score.json', \"w\") as writer:\n",
    "#     writer.write(json.dumps(save_score, indent=4, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "quries = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba2472d-3222-4357-903f-a63fd8e9ab60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#datafram을 pickle로 top5 저장\n",
    "import pandas as pd\n",
    "total = []\n",
    "docs = []\n",
    "quries = []\n",
    "for idx, example in enumerate(tqdm(testset, desc=\"elasticsearch: \")):\n",
    "    # relev_doc_ids = [el for i, el in enumerate(self.ids) if i in doc_indices[idx]]\n",
    "    question=example[\"question\"]\n",
    "    top_k_docs = pipe.run(question, params={\"retriever\": {\"top_k\": 30}})\n",
    "\n",
    "    # query = {\n",
    "    #     'query':{\n",
    "    #         'bool':{\n",
    "    #             'must':[\n",
    "    #                       {'match':{'text':question}}\n",
    "    #             ],\n",
    "    #             'should':[\n",
    "    #                       {'match':{'text':question}}\n",
    "    #             ]\n",
    "    #         }\n",
    "    #     }\n",
    "    # }\n",
    "\n",
    "    query = {\n",
    "            'query': {\n",
    "                'match': {\n",
    "                    'text': question\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "    quries.append(query)\n",
    "    \n",
    "    doc = es.search(index='document',body=query,size=30)['hits']['hits']\n",
    "    cc = ''\n",
    "    docs.append(doc)\n",
    "    \n",
    "    for i in range(10):\n",
    "        cc += doc[i]['_source']['text']\n",
    "        \n",
    "\n",
    "    tmp = {\n",
    "        \"question\": example[\"question\"],\n",
    "        \"id\": example['id'],\n",
    "        \"context_id\": doc[0]['_id'],  # retrieved id\n",
    "        \"context\": cc # doc[0]['_source']['text']+doc[1]['_source']['text']+doc[2]['_source']['text']+doc[3]['_source']['text']+doc[4]['_source']['text'] # retrieved doument\n",
    "    }\n",
    " \n",
    "    if 'context' in example.keys() and 'answers' in example.keys():\n",
    "        tmp[\"original_context\"] = example['context']  # original document\n",
    "        tmp[\"answers\"] = example['answers']           # original answer\n",
    "    total.append(tmp)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
