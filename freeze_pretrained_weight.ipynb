{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import wandb\n",
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from importlib import import_module\n",
    "\n",
    "from arguments import (\n",
    "    DefaultArguments,\n",
    "    DatasetArguments,\n",
    "    ModelArguments,\n",
    "    RetrieverArguments,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    set_seed,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    DataCollatorWithPadding,\n",
    "    AdamW,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    get_cosine_with_hard_restarts_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from datasets import load_from_disk, load_metric\n",
    "\n",
    "from preprocessor import BaselinePreprocessor\n",
    "from postprocessor import post_processing_function\n",
    "from model.models import BaseModel\n",
    "from retrieval import SparseRetrieval\n",
    "from utils import increment_path, LossObject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    \"\"\"argument 객체 생성 함수\"\"\"\n",
    "    parser = HfArgumentParser(\n",
    "        (DefaultArguments, DatasetArguments, ModelArguments, RetrieverArguments, TrainingArguments)\n",
    "    )\n",
    "    default_args, dataset_args, model_args, retriever_args, training_args = parser.parse_args_into_dataclasses()\n",
    "    return default_args, dataset_args, model_args, retriever_args, training_args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_save_path(training_args):\n",
    "    \"\"\"모델 및 로그 저장 경로 생성 함수\"\"\"\n",
    "    training_args.output_dir = os.path.join(training_args.output_dir, training_args.run_name)\n",
    "    training_args.output_dir = increment_path(\n",
    "        training_args.output_dir, training_args.overwrite_output_dir\n",
    "    )\n",
    "    training_args.logging_dir = increment_path(\n",
    "        training_args.logging_dir, training_args.overwrite_output_dir\n",
    "    )\n",
    "    print(f\"output_dir : {training_args.output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def set_logging(default_args, dataset_args, model_args, retriever_args, training_args):\n",
    "    \"\"\"logging setting 함수\"\"\"\n",
    "    logging_level_dict = {\n",
    "        \"DEBUG\": logging.DEBUG,         # 문제를 해결할 때 필요한 자세한 정보\n",
    "        \"INFO\": logging.INFO,           # 작업이 정상적으로 작동하고 있다는 확인 메시지\n",
    "        \"WARNING\": logging.WARNING,     # 예상하지 못한 일이 발생 or 발생 가능한 문제점을 명시\n",
    "        \"ERROR\": logging.ERROR,         # 프로그램이 함수를 실행하지 못 할 정도의 심각한 문제\n",
    "        \"CRITICAL\": logging.CRITICAL    # 프로그램이 동작할 수 없을 정도의 심각한 문제\n",
    "    }\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(module)s - %(levelname)s  %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "        level=logging_level_dict[default_args.log_level]\n",
    "    )\n",
    "    logger.debug(\"Default arguments %s\", default_args)\n",
    "    logger.debug(\"Dataset arguments %s\", dataset_args)\n",
    "    logger.debug(\"Model arguments %s\", model_args)\n",
    "    logger.debug(\"Retriever arguments %s\", retriever_args)\n",
    "    logger.debug(\"Training argumenets %s\", training_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed_everything(seed):\n",
    "    '''seed 고정 함수'''\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_parameters(model, training_args):\n",
    "    \"\"\"weight decay가 적용된 모델 파라미터 생성 함수\"\"\"\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [param for name, param in model.named_parameters() if not any(nd in name for nd in no_decay)],\n",
    "            \"weight_decay\": training_args.weight_decay\n",
    "        },\n",
    "        {\n",
    "            \"params\": [param for name, param in model.named_parameters() if any(nd in name for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0\n",
    "        }\n",
    "    ]\n",
    "    return grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model(model_args, training_args):\n",
    "    \"\"\"model, tokenizer, optimizer 객체 생성 함수\"\"\"\n",
    "    # model config\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config if model_args.config is not None else model_args.model\n",
    "    )\n",
    "\n",
    "    # tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_args.tokenizer if model_args.tokenizer is not None else model_args.model\n",
    "    )\n",
    "\n",
    "    # model\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "        model_args.model, from_tf=bool(\".ckpt\" in model_args.model), config=config\n",
    "    )\n",
    "    # TODO: load custom model here\n",
    "    #model.qa_outputs = CustomModel(config)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = AdamW(\n",
    "        params=get_grouped_parameters(model, training_args),\n",
    "        lr=training_args.learning_rate,\n",
    "        eps=training_args.adam_epsilon\n",
    "    )\n",
    "\n",
    "    return config, model, tokenizer, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(dataset_args, training_args, tokenizer):\n",
    "    \"\"\"데이터셋 생성, preprocess 적용, dataLoader 객체 생성 함수\"\"\"\n",
    "    datasets = load_from_disk(dataset_args.dataset_path)\n",
    "    train_dataset = datasets['train']\n",
    "    eval_dataset = datasets['validation'] \n",
    "    eval_dataset_for_predict = datasets['validation']\n",
    "    column_names = train_dataset.column_names\n",
    "\n",
    "    preprocessor = BaselinePreprocessor(\n",
    "        dataset_args=dataset_args, tokenizer=tokenizer, column_names=column_names\n",
    "    )\n",
    "    # 모델 학습 및 training loss 계산을 위한 dataset\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocessor.prepare_train_features,\n",
    "        batched=True,\n",
    "        num_proc=dataset_args.num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not dataset_args.overwrite_cache,\n",
    "    )\n",
    "    # 모델 평가 및 eval loss 계산을 위한 dataset\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        preprocessor.prepare_train_features,\n",
    "        batched=True,\n",
    "        num_proc=dataset_args.num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not dataset_args.overwrite_cache,\n",
    "    )\n",
    "    # evaluation(validation) 데이터셋에 대한 예측값 생성 및 평가지표 계산을 위한 dataset\n",
    "    eval_dataset_for_predict = eval_dataset_for_predict.map(\n",
    "        preprocessor.prepare_eval_features,\n",
    "        batched=True,\n",
    "        num_proc=dataset_args.num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not dataset_args.overwrite_cache,\n",
    "    )    \n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        collate_fn = data_collator,\n",
    "        batch_size=training_args.per_device_train_batch_size\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        collate_fn = data_collator,\n",
    "        batch_size=training_args.per_device_eval_batch_size\n",
    "    )\n",
    "\n",
    "    return datasets, eval_dataset_for_predict, train_dataloader, eval_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_scheduler(optimizer, train_dataloader, training_args):\n",
    "    num_training_steps = len(train_dataloader) // training_args.gradient_accumulation_steps * training_args.num_train_epochs\n",
    "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=training_args.warmup_steps,\n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def concat_context_logits(logits, dataset, max_len):\n",
    "    \"\"\"각 batch의 logits을 하나로 합쳐주는 함수\"\"\"\n",
    "    step = 0\n",
    "    logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n",
    "\n",
    "    for i, output_logit in enumerate(logits):\n",
    "        batch_size = output_logit.shape[0]\n",
    "        cols = output_logit.shape[1]\n",
    "        if step + batch_size < len(dataset):\n",
    "            logits_concat[step : step + batch_size, :cols] = output_logit\n",
    "        else:\n",
    "            logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n",
    "        step += batch_size\n",
    "\n",
    "    return logits_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_step(model, optimizer, scheduler, batch, device):\n",
    "    \"\"\"각 training batch에 대한 모델 학습 및 train loss 계산 함수\"\"\"\n",
    "    model.train()\n",
    "    batch = batch.to(device)\n",
    "    outputs = model(**batch)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluation_step(model, datasets, eval_dataset_for_predict, eval_dataloader, dataset_args, training_args, device):\n",
    "    \"\"\"모든 evaluation dataset에 대한 loss 및 metric 계산 함수\"\"\"\n",
    "    metric = load_metric(\"squad\")\n",
    "    model.eval()\n",
    "\n",
    "    start_logits_list = []\n",
    "    end_logits_list = []\n",
    "\n",
    "    loss = 0\n",
    "    eval_num = 0\n",
    "    for batch in eval_dataloader:\n",
    "        batch = batch.to(device)\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss += outputs.loss\n",
    "        eval_num += len(batch['input_ids'])\n",
    "\n",
    "        start_logits = outputs['start_logits'] # (batch_size, token_num)\n",
    "        end_logits = outputs['end_logits'] # (batch_size, token_num)\n",
    "        start_logits_list.append(start_logits.detach().cpu().numpy())\n",
    "        end_logits_list.append(end_logits.detach().cpu().numpy())\n",
    "\n",
    "    max_len = max(x.shape[1] for x in start_logits_list)\n",
    "\n",
    "    start_logits_concat = concat_context_logits(start_logits_list, eval_dataset_for_predict, max_len)\n",
    "    end_logits_concat = concat_context_logits(end_logits_list, eval_dataset_for_predict, max_len)\n",
    "\n",
    "    eval_dataset_for_predict.set_format(type=None, columns=list(eval_dataset_for_predict.features.keys()))\n",
    "    predictions = (start_logits_concat, end_logits_concat)\n",
    "    eval_preds = post_processing_function(datasets['validation'], eval_dataset_for_predict, datasets, predictions, training_args, dataset_args)\n",
    "    eval_metric = metric.compute(predictions=eval_preds.predictions, references=eval_preds.label_ids) # compute_metrics\n",
    "    \n",
    "    return eval_metric, loss, eval_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_mrc(\n",
    "    default_args, dataset_args, model_args, retriever_args, training_args,\n",
    "    model, optimizer, scheduler, tokenizer,\n",
    "    datasets, eval_dataset_for_predict,\n",
    "    train_dataloader, eval_dataloader,\n",
    "    device\n",
    "):\n",
    "    \"\"\"MRC 모델 학습 및 평가 함수\"\"\"\n",
    "    prev_eval_loss = float('inf')\n",
    "    global_steps = 0\n",
    "    train_loss_obj = LossObject()\n",
    "    eval_loss_obj = LossObject()\n",
    "    for epoch in range(int(training_args.num_train_epochs)):\n",
    "        pbar = tqdm(\n",
    "            enumerate(train_dataloader),\n",
    "            total=len(train_dataloader),\n",
    "            position=0,\n",
    "            leave=True\n",
    "        )\n",
    "        for step, batch in pbar:\n",
    "            loss = train_step(model, optimizer, scheduler, batch, device)\n",
    "\n",
    "            global_steps += 1\n",
    "            train_loss_obj.update(loss, len(batch['input_ids']))\n",
    "\n",
    "            description = f\"epoch: {epoch+1:03d} | step: {global_steps:05d} | train loss: {train_loss_obj.get_avg_loss():.4f}\"\n",
    "            pbar.set_description(description)\n",
    "\n",
    "            if global_steps % training_args.eval_steps == 0:\n",
    "                with torch.no_grad():\n",
    "                    eval_metric, eval_loss, eval_num = evaluation_step(model, datasets, eval_dataset_for_predict, eval_dataloader, dataset_args, training_args, device)\n",
    "\n",
    "                eval_loss_obj.update(eval_loss, eval_num)\n",
    "\n",
    "                if eval_loss_obj.get_avg_loss() < prev_eval_loss:\n",
    "                    # TODO: 5개 저장됐을 때 삭제하는 로직 개발 필요 -> huggingface format 모델 저장 필요\n",
    "                    model.save_pretrained(os.path.join(training_args.output_dir, f\"checkpoint-{global_steps:05d}\"))\n",
    "                    prev_eval_loss = eval_loss_obj.get_avg_loss()\n",
    "                # TODO: 하이퍼파라미터(arguments) 정보 wandb에 기록하는 로직 필요\n",
    "                wandb.log({\n",
    "                    'global_steps': global_steps,\n",
    "                    'train/loss': train_loss_obj.get_avg_loss(),\n",
    "                    'train/learning_rate': training_args.learning_rate,\n",
    "                    'eval/loss': eval_loss_obj.get_avg_loss(),\n",
    "                    'eval/exact_match' : eval_metric['exact_match'],\n",
    "                    'eval/f1_score' : eval_metric['f1']\n",
    "                })\n",
    "                train_loss_obj.reset()\n",
    "                eval_loss_obj.reset()            \n",
    "                \n",
    "            else:\n",
    "                wandb.log({'global_steps':global_steps})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    default_args, dataset_args, model_args, retriever_args, training_args = get_args()\n",
    "    set_logging(default_args, dataset_args, model_args, retriever_args, training_args)\n",
    "    update_save_path(training_args)\n",
    "    set_seed_everything(training_args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    config, model, tokenizer, optimizer = get_model(model_args, training_args)\n",
    "    model.to(device)\n",
    "\n",
    "    datasets, eval_dataset_for_predict, train_dataloader, eval_dataloader = get_data(dataset_args, training_args, tokenizer)\n",
    "\n",
    "    scheduler = get_scheduler(optimizer, train_dataloader, training_args)\n",
    "\n",
    "    # set wandb\n",
    "    wandb.login()\n",
    "    wandb.init(\n",
    "        project=default_args.wandb_project,\n",
    "        entity=default_args.wandb_entity,\n",
    "        name=training_args.run_name\n",
    "    )\n",
    "\n",
    "    train_mrc(\n",
    "        default_args, dataset_args, model_args, retriever_args, training_args,\n",
    "        model, optimizer, scheduler, tokenizer,\n",
    "        datasets, eval_dataset_for_predict,\n",
    "        train_dataloader, eval_dataloader,\n",
    "        device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--log_level LOG_LEVEL]\n",
      "                             [--wandb_entity WANDB_ENTITY]\n",
      "                             [--wandb_project WANDB_PROJECT]\n",
      "                             [--dataset_path DATASET_PATH]\n",
      "                             [--max_seq_len MAX_SEQ_LEN]\n",
      "                             [--stride_len STRIDE_LEN]\n",
      "                             [--max_ans_len MAX_ANS_LEN]\n",
      "                             [--use_max_padding [USE_MAX_PADDING]]\n",
      "                             [--use_bucketing [USE_BUCKETING]]\n",
      "                             [--num_workers NUM_WORKERS]\n",
      "                             [--no_overwrite_cache]\n",
      "                             [--overwrite_cache [OVERWRITE_CACHE]]\n",
      "                             [--model MODEL] [--config CONFIG]\n",
      "                             [--tokenizer TOKENIZER] [--head HEAD]\n",
      "                             [--loss_fn {CrossEntropyLoss,BCELoss,MSELoss,L1Loss}]\n",
      "                             [--retriver RETRIVER]\n",
      "                             [--top_k_retrieval TOP_K_RETRIEVAL]\n",
      "                             [--no_use_eval_retrieval]\n",
      "                             [--use_eval_retrieval [USE_EVAL_RETRIEVAL]]\n",
      "                             [--use_faiss [USE_FAISS]]\n",
      "                             [--num_clusters NUM_CLUSTERS] --output_dir\n",
      "                             OUTPUT_DIR\n",
      "                             [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n",
      "                             [--do_train [DO_TRAIN]] [--do_eval [DO_EVAL]]\n",
      "                             [--do_predict [DO_PREDICT]]\n",
      "                             [--evaluation_strategy {no,steps,epoch}]\n",
      "                             [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n",
      "                             [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n",
      "                             [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n",
      "                             [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n",
      "                             [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "                             [--weight_decay WEIGHT_DECAY]\n",
      "                             [--adam_beta1 ADAM_BETA1]\n",
      "                             [--adam_beta2 ADAM_BETA2]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_steps MAX_STEPS]\n",
      "                             [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n",
      "                             [--warmup_ratio WARMUP_RATIO]\n",
      "                             [--warmup_steps WARMUP_STEPS]\n",
      "                             [--logging_dir LOGGING_DIR]\n",
      "                             [--logging_strategy {no,steps,epoch}]\n",
      "                             [--logging_first_step [LOGGING_FIRST_STEP]]\n",
      "                             [--logging_steps LOGGING_STEPS]\n",
      "                             [--save_strategy {no,steps,epoch}]\n",
      "                             [--save_steps SAVE_STEPS]\n",
      "                             [--save_total_limit SAVE_TOTAL_LIMIT]\n",
      "                             [--no_cuda [NO_CUDA]] [--seed SEED]\n",
      "                             [--fp16 [FP16]] [--fp16_opt_level FP16_OPT_LEVEL]\n",
      "                             [--fp16_backend {auto,amp,apex}]\n",
      "                             [--fp16_full_eval [FP16_FULL_EVAL]]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "                             [--tpu_num_cores TPU_NUM_CORES]\n",
      "                             [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n",
      "                             [--debug [DEBUG]]\n",
      "                             [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n",
      "                             [--eval_steps EVAL_STEPS]\n",
      "                             [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                             [--past_index PAST_INDEX] [--run_name RUN_NAME]\n",
      "                             [--disable_tqdm DISABLE_TQDM]\n",
      "                             [--no_remove_unused_columns]\n",
      "                             [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n",
      "                             [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n",
      "                             [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n",
      "                             [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n",
      "                             [--greater_is_better GREATER_IS_BETTER]\n",
      "                             [--ignore_data_skip [IGNORE_DATA_SKIP]]\n",
      "                             [--sharded_ddp SHARDED_DDP]\n",
      "                             [--deepspeed DEEPSPEED]\n",
      "                             [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n",
      "                             [--adafactor [ADAFACTOR]]\n",
      "                             [--group_by_length [GROUP_BY_LENGTH]]\n",
      "                             [--length_column_name LENGTH_COLUMN_NAME]\n",
      "                             [--report_to REPORT_TO [REPORT_TO ...]]\n",
      "                             [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n",
      "                             [--no_dataloader_pin_memory]\n",
      "                             [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n",
      "                             [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n",
      "                             [--mp_parameters MP_PARAMETERS]\n",
      "ipykernel_launcher.py: error: ambiguous option: --f=/tmp/tmp-28352w70iM3WpaFV2.json could match --fp16, --fp16_opt_level, --fp16_backend, --fp16_full_eval\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
