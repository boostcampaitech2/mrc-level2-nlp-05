{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoConfig, AutoTokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47563bed2474a408845b3908e2c538e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e26172f367c44e3aa502dc8a435b8fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/495k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"klue/bert-base\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_from_disk(\"/opt/ml/data/train_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['title', 'context', 'question', 'id', 'answers', 'document_id', '__index_level_0__'],\n",
      "    num_rows: 3952\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets[\"train\"]\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_start': [235], 'text': ['하원']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"answers\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = tokenizer(\n",
    "    train_dataset[\"question\"][0],\n",
    "    train_dataset[\"context\"][0],\n",
    "    return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-df2e468ad993>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m235\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m238\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m out = model(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mtokenized_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'labels'"
     ]
    }
   ],
   "source": [
    "start_positions = torch.tensor([235])\n",
    "end_positions = torch.tensor([238])\n",
    "labels = torch.tensor([[235, 238]])\n",
    "\n",
    "out = model(\n",
    "    **tokenized_sentence, \n",
    "    start_positions=start_positions, \n",
    "    end_positions=end_positions,\n",
    "    return_dict=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuestionAnsweringModelOutput(loss=tensor(6.0589, grad_fn=<DivBackward0>), start_logits=tensor([[-0.3059, -0.9222, -0.2331, -0.8421, -0.9025, -1.1790, -0.7388, -0.3408,\n",
       "         -0.1035, -0.5049, -0.2560,  0.0613, -0.5251, -0.2891, -0.7815,  0.2120,\n",
       "         -0.6595, -0.2199, -1.0516, -0.5320, -0.0349, -0.0700, -0.9708, -0.2157,\n",
       "         -0.2859,  0.7928,  0.6218, -0.2001, -0.3493, -0.3769, -0.3863, -0.0763,\n",
       "          0.2019,  0.0544, -0.1868,  0.2715,  0.1691, -0.8376, -0.1143, -0.3292,\n",
       "         -0.7497,  0.3955, -0.0778,  0.0379, -0.6412, -0.0810, -0.5769, -0.2044,\n",
       "         -0.0623, -0.6679, -0.1289, -0.4068,  0.0559, -0.5087, -0.4809, -0.5339,\n",
       "         -0.1168,  0.0655, -0.1178,  0.0824, -0.2020, -0.2079,  0.2262,  0.1166,\n",
       "         -0.5492, -0.1458, -0.2022, -0.0715,  0.2656, -0.1173, -0.5548,  0.0815,\n",
       "          0.0676, -0.6508, -0.0288, -0.0987, -1.2160,  0.1511, -0.2197,  1.0742,\n",
       "          0.0835,  0.1725,  0.1395, -0.1228, -0.1613, -0.7191, -0.8271, -0.2049,\n",
       "          0.3245,  0.1869,  0.3278,  0.7360, -0.4494, -0.2126, -0.4709, -0.1536,\n",
       "          0.4285, -0.7487, -0.1456, -0.2630,  0.2189, -0.5875, -0.4081, -0.4649,\n",
       "          0.0723,  0.0883, -0.2207,  0.0839, -0.6837, -0.1616, -0.8074, -0.1135,\n",
       "         -0.0750, -0.8709,  0.0512, -0.8805, -0.5919,  0.5316,  0.1194, -0.4988,\n",
       "         -0.7150, -1.1273, -1.1196, -0.2208, -1.3100, -0.9593, -0.8167, -1.1924,\n",
       "         -1.2748, -0.6727, -0.3647, -0.1269, -0.2040, -0.4468, -0.6482, -0.7233,\n",
       "         -0.8916, -1.1054,  0.1575, -0.2188, -0.7687, -0.1232, -0.4544, -0.3439,\n",
       "         -1.1895, -0.1789, -1.0160, -0.6628, -0.0280, -0.6008, -0.1528, -0.6170,\n",
       "         -0.6456, -0.1549, -0.1729, -0.4276, -0.1189, -0.8321, -1.1401, -0.8286,\n",
       "          0.0414, -0.0359, -0.2402, -0.8595, -0.4757, -0.5247, -0.2787, -0.5520,\n",
       "         -0.1077, -0.7356, -0.7206,  0.4098,  0.0968, -0.8311,  0.0621, -1.0476,\n",
       "         -0.3199,  0.0914, -0.4777, -0.7487, -0.0923, -0.2200, -0.0141,  0.9829,\n",
       "         -0.2779, -0.2403, -0.1667,  0.9649,  0.4423,  0.2831, -0.3169, -0.0832,\n",
       "          0.1340, -0.1660, -0.3705, -0.7448, -0.2545, -0.7973, -0.8702,  0.0701,\n",
       "         -0.2185, -0.3078, -0.0582, -0.3543, -0.1964, -0.5158, -0.6985, -0.7649,\n",
       "         -0.5824, -0.5428, -1.1474, -0.1211, -0.7162, -0.6372, -0.7229, -0.2747,\n",
       "         -0.4806, -0.3045, -1.1645, -1.0291, -0.0698, -0.9888, -0.3752,  0.4812,\n",
       "         -0.9158, -0.4563,  0.7075,  0.3425, -0.8555, -0.0339, -0.6017, -0.4509,\n",
       "         -1.3165, -0.1083, -0.2188, -0.2509, -0.7978, -0.0529, -0.9255, -0.0163,\n",
       "         -0.7471, -0.2900, -0.8381, -0.4087, -0.3250,  0.0464, -1.1017, -1.2764,\n",
       "         -0.7417, -0.2774, -0.1177, -0.6183, -1.3649, -0.0569, -0.2124, -0.6779,\n",
       "          0.0708, -0.2617,  0.7430, -0.1948,  0.0201, -0.1359, -0.9385, -0.5939,\n",
       "         -0.8153, -0.1755,  0.0366,  0.2923, -0.2634, -0.2991, -1.2277,  0.1053,\n",
       "         -0.0825, -0.1682, -0.9237, -0.2481, -0.6030,  0.3007,  0.0286,  0.1744,\n",
       "         -1.4241,  0.2822, -0.2186,  0.6399,  0.1375,  1.2535,  0.6226, -0.0659,\n",
       "          0.4101, -0.0470, -0.3769, -0.5038, -0.0177, -0.9966, -0.5064, -0.5995,\n",
       "         -0.3827, -0.3750, -0.1328, -0.2919, -0.8667, -0.3786, -0.0302,  0.3566,\n",
       "         -0.0093,  0.0398, -0.0580, -0.2165, -0.3496, -0.5722, -0.3330,  0.7003,\n",
       "         -0.3371,  0.0598,  0.5062, -1.0213, -0.5503,  0.5691, -0.8183, -0.8378,\n",
       "         -0.0476, -0.1337, -0.2191, -0.4210, -0.3975, -1.0368, -0.3346,  0.5133,\n",
       "         -0.0744, -0.1544, -0.9082, -0.0383,  0.6801,  0.3382, -0.2171,  0.4026,\n",
       "          0.9817,  0.2593,  0.4612,  0.2154,  0.0425,  0.1277, -0.4844, -0.2341,\n",
       "         -0.9726,  0.4051,  0.1606, -1.4284, -0.5877,  0.4489, -0.6717, -0.5102,\n",
       "          0.4751, -0.2184, -0.4470,  0.7674,  0.8618, -0.8493,  0.4165, -0.7670,\n",
       "         -0.0889, -1.0475, -0.6657, -0.7950,  0.4974,  0.4467,  0.2439,  0.1865,\n",
       "         -0.5552, -0.1779,  0.1387,  0.0376,  0.3216,  0.1816, -0.1971,  0.8208,\n",
       "          0.2458, -0.0596,  0.3598, -0.0126, -0.1586, -0.0390,  0.3245, -0.6118,\n",
       "          0.1626,  0.6007,  0.5468, -0.0174,  0.1044, -0.5215, -0.8864,  0.3310,\n",
       "         -0.2196, -0.6633,  0.1486, -0.4350, -0.3490, -0.5738,  0.1906, -0.5806,\n",
       "          0.4677,  0.5443,  0.7954, -0.6803, -0.1627, -0.9940, -0.4362, -0.7589,\n",
       "          0.3809,  0.3696,  0.2419, -0.6496, -0.7540,  0.0498, -0.1532, -0.6997,\n",
       "         -0.0385, -0.7025, -0.3107,  0.1198, -0.8144,  0.1661,  0.3726, -0.3817,\n",
       "          0.6680, -0.1658, -0.1137,  0.0140, -0.5395,  0.4004,  0.0207,  0.5014,\n",
       "          0.2563,  0.2033,  0.2726, -0.5443, -0.4401,  0.4366,  0.2390,  0.2370,\n",
       "          0.2377, -0.4717, -0.4122,  0.5231, -0.2159,  0.4059, -0.0939,  0.4096,\n",
       "         -0.0644,  0.6075, -0.3667,  1.5027, -0.2198]],\n",
       "       grad_fn=<SqueezeBackward1>), end_logits=tensor([[-9.4347e-01, -1.5092e-01, -9.6781e-01, -1.1020e+00, -1.3550e-01,\n",
       "         -8.9014e-01,  9.8110e-02, -1.0834e-01,  1.6045e-01, -6.0061e-01,\n",
       "         -9.9378e-01, -7.2063e-01,  1.5506e-01,  4.1881e-01,  2.7279e-01,\n",
       "         -4.5960e-02,  4.9988e-01, -5.8240e-01, -6.5318e-01,  2.3158e-01,\n",
       "         -6.8272e-01, -3.6054e-01, -7.1390e-01, -1.0035e+00, -2.6756e-01,\n",
       "          5.2382e-02, -3.8119e-01, -7.7050e-02,  1.4947e-01, -3.0628e-02,\n",
       "         -8.2263e-01, -2.8212e-01, -4.6570e-01,  3.0709e-01, -4.7561e-01,\n",
       "         -8.2475e-01, -4.5909e-01, -5.5784e-01, -6.8680e-02,  4.1813e-01,\n",
       "         -8.9596e-01, -7.1559e-01,  2.4287e-01, -2.1942e-01, -3.7928e-01,\n",
       "         -3.0679e-01,  4.3532e-01, -9.0698e-01, -5.7892e-01, -6.9524e-01,\n",
       "         -6.8974e-01, -4.9815e-01, -9.1873e-02, -8.3762e-01, -2.0196e-01,\n",
       "         -2.4033e-01,  7.3905e-02, -1.3681e-01, -9.6807e-02, -1.0412e+00,\n",
       "         -4.7057e-01, -1.0559e+00, -1.2878e+00, -9.0273e-01, -7.3241e-01,\n",
       "          1.3537e-02,  3.5120e-01,  5.5487e-02, -7.3251e-01, -2.4783e-01,\n",
       "         -6.6564e-01, -1.1986e+00, -6.8587e-01, -3.1546e-01,  1.2650e-01,\n",
       "          1.8505e-01, -2.4227e-01, -8.7726e-02, -5.8152e-01,  4.1082e-02,\n",
       "          3.1298e-01,  5.5595e-01,  3.5821e-01, -1.1712e-01,  2.1985e-01,\n",
       "          3.2598e-01,  6.9153e-01,  2.4072e-01, -3.5717e-01,  5.5550e-01,\n",
       "          3.5918e-04, -3.1902e-01,  2.9555e-01, -3.3140e-01,  1.0866e-01,\n",
       "          3.0663e-01, -1.6557e-01, -7.2400e-01, -1.0671e+00, -4.9228e-01,\n",
       "         -8.9403e-03, -3.6075e-01,  3.2421e-01,  6.4236e-01,  3.4503e-01,\n",
       "          1.8038e-01, -3.4369e-01, -4.4395e-01, -4.3547e-01, -3.4211e-01,\n",
       "          2.3039e-01, -7.0291e-01, -3.4641e-01, -5.3399e-01, -5.4389e-01,\n",
       "         -3.4912e-01,  8.4397e-03,  1.2596e-02,  1.7924e-01,  5.5453e-01,\n",
       "          2.4518e-01, -4.8812e-01,  1.2250e-01, -8.7082e-01, -3.3597e-01,\n",
       "         -5.6236e-01,  1.4151e-01,  4.7919e-01, -3.6174e-01,  3.1579e-01,\n",
       "          3.7688e-01,  4.0296e-01,  4.4723e-01,  1.1451e+00,  5.5736e-02,\n",
       "          4.0395e-01,  5.9196e-01,  4.1345e-03, -6.0133e-01, -5.7137e-01,\n",
       "         -2.8927e-01, -5.0878e-01,  1.3539e-01,  1.2496e-01,  2.4996e-01,\n",
       "          2.6752e-01,  7.3636e-01,  4.5130e-01, -2.9736e-01,  3.3702e-01,\n",
       "         -6.0867e-01, -5.7904e-01,  1.3522e-01, -2.1298e-01,  1.0979e-01,\n",
       "          1.0191e-01, -1.4735e-01,  2.0806e-01,  2.8270e-01, -1.7674e-01,\n",
       "         -7.2231e-01, -8.7480e-01, -6.1286e-01, -6.6094e-01,  3.4628e-01,\n",
       "         -7.2263e-01, -8.3939e-01,  2.0615e-01,  3.7070e-02,  2.8738e-01,\n",
       "         -3.0889e-02, -3.2530e-01,  1.2258e+00, -3.8537e-01, -4.2432e-01,\n",
       "         -4.2362e-01,  3.6544e-01,  2.7054e-01,  1.4259e-01,  5.2551e-01,\n",
       "         -1.2025e-01, -5.7952e-01,  1.8773e-01, -4.0622e-01,  2.6216e-01,\n",
       "         -1.2458e-01,  3.8796e-01,  1.2178e-01,  2.8457e-01,  1.1818e-01,\n",
       "          4.6738e-01,  3.0392e-01,  1.9679e-01,  2.5425e-01,  4.1509e-02,\n",
       "          3.4892e-01,  2.1813e-01,  3.3430e-01, -1.2916e-01, -8.5283e-01,\n",
       "         -5.5713e-01, -2.4746e-01, -6.6225e-02, -1.8344e-02,  3.7767e-01,\n",
       "          5.2335e-01,  2.6472e-01, -1.2852e-01,  8.4529e-01,  2.9628e-01,\n",
       "          1.8114e-02, -3.8024e-01,  1.8223e-01,  1.0245e+00, -2.1375e-01,\n",
       "          3.9001e-01,  4.7096e-01, -1.4753e-01,  1.5969e-01,  9.5394e-01,\n",
       "         -5.1111e-01, -1.8745e-01, -2.4766e-01, -3.4264e-01,  3.9497e-01,\n",
       "         -3.1582e-02, -6.4432e-01,  1.1574e-01, -3.6070e-01, -4.4020e-01,\n",
       "         -8.9254e-02, -5.9646e-01, -4.5117e-01, -1.3814e-01, -5.8444e-01,\n",
       "          3.0786e-01,  7.7423e-02, -3.2374e-01,  2.3376e-01,  3.1013e-01,\n",
       "         -4.5679e-02, -5.7588e-01,  3.0121e-01,  3.1733e-01,  5.4026e-01,\n",
       "         -2.8384e-01,  8.8440e-01,  1.5504e-01, -8.7192e-02, -8.6087e-01,\n",
       "         -5.3251e-01, -8.3174e-01, -5.5599e-01, -4.1134e-01, -5.3140e-01,\n",
       "          6.0220e-01,  4.1657e-02, -2.2273e-01,  4.9278e-02, -4.4104e-01,\n",
       "         -4.6842e-01, -7.5070e-02,  1.1893e+00, -3.4623e-02, -1.2369e-01,\n",
       "          3.8442e-01, -4.8417e-02,  1.5622e-01,  4.8448e-01,  5.3839e-01,\n",
       "         -2.6286e-01, -7.6210e-01,  2.6490e-01,  2.5194e-01,  8.1391e-01,\n",
       "          1.4162e-02, -1.5903e-01, -7.9781e-01, -1.7784e-01,  4.0386e-01,\n",
       "         -4.0917e-01,  9.0877e-02, -5.8299e-01,  6.3663e-01,  1.3936e-01,\n",
       "          2.8919e-01, -2.8847e-02,  8.0264e-01, -2.9722e-01,  3.0001e-01,\n",
       "         -5.8002e-01, -4.4122e-01, -5.9117e-01, -2.6206e-01,  4.7582e-01,\n",
       "          1.1063e+00,  5.4399e-01,  5.9570e-02,  8.5609e-02,  2.3622e-01,\n",
       "         -4.9208e-02, -1.7485e-01,  7.0492e-01,  1.3238e-01,  2.8860e-01,\n",
       "         -4.5742e-01,  4.3863e-01,  4.5953e-01, -1.6348e-01,  1.2377e-03,\n",
       "         -5.8091e-01, -1.0675e+00, -4.0055e-01,  4.9740e-01,  1.1948e+00,\n",
       "         -3.1508e-01, -6.0719e-01, -2.1835e-01, -7.6999e-02, -7.6592e-01,\n",
       "         -8.3086e-01,  4.0545e-02,  2.8310e-01, -5.5729e-02, -1.1915e+00,\n",
       "         -5.7082e-01, -5.5081e-01,  6.7906e-01, -4.8851e-02, -7.5263e-02,\n",
       "         -7.3714e-01, -7.8676e-01, -1.0537e+00,  8.4447e-02,  3.7715e-01,\n",
       "          2.5444e-01,  4.2001e-01,  7.0103e-01,  1.5762e-01,  2.9260e-01,\n",
       "         -4.3447e-01,  2.2075e-01,  4.0107e-01, -3.1006e-01, -1.3294e-01,\n",
       "         -2.3103e-01,  1.1725e-01, -1.2279e-01, -1.9627e-01,  3.2076e-01,\n",
       "          4.0910e-01, -2.3157e-01, -4.6912e-01, -5.8529e-01, -2.8442e-01,\n",
       "         -7.0288e-01, -4.4457e-01, -6.7099e-01, -7.8475e-01, -5.9290e-01,\n",
       "         -1.0628e+00, -7.9633e-02, -4.4510e-01, -8.6728e-03, -8.9580e-01,\n",
       "         -1.8739e-01, -4.3046e-01,  3.6008e-01,  1.8145e-01, -2.2829e-01,\n",
       "         -4.0374e-01, -4.4399e-02, -1.0065e+00, -5.1070e-01, -2.7201e-02,\n",
       "          2.3923e-01, -3.6468e-01, -8.5394e-01, -1.5011e-01,  1.0772e-01,\n",
       "         -1.8320e-02, -1.3144e-01,  2.6949e-02, -2.4391e-01, -5.9026e-01,\n",
       "         -1.3544e+00, -5.6719e-01, -1.7061e-01, -6.8203e-02,  1.5788e-01,\n",
       "         -2.2197e-01, -7.2339e-01, -5.8192e-01, -4.8458e-01, -6.7049e-01,\n",
       "          1.1165e-01,  1.9242e-01,  3.6592e-01, -5.5742e-01, -5.0390e-01,\n",
       "         -5.0631e-01, -3.4717e-01, -4.5944e-01,  2.9699e-02, -6.3250e-01,\n",
       "          6.2285e-01,  2.0239e-01,  3.9961e-01, -5.1534e-01, -9.3037e-02,\n",
       "         -2.9205e-01,  4.7514e-01, -4.8494e-01, -9.0921e-01, -1.3034e-01,\n",
       "         -9.8465e-02, -7.2710e-01,  3.4590e-01,  4.8829e-01,  5.4150e-02,\n",
       "         -2.1181e-01, -1.3063e-01,  4.4443e-01,  2.7958e-01, -1.1358e-01,\n",
       "         -4.7669e-01, -2.4986e-01, -4.0706e-01, -5.1366e-01, -1.0549e+00,\n",
       "         -3.2666e-02, -7.1867e-01, -3.0042e-02,  2.2346e-01,  3.1961e-01,\n",
       "         -1.5965e-02, -2.6910e-01, -6.9671e-01,  1.7641e-01,  4.5959e-02,\n",
       "         -1.4123e-01,  2.6232e-01,  1.8447e-01, -2.4712e-01, -5.7816e-01,\n",
       "          3.2469e-01, -6.8687e-01,  1.0505e-01, -6.5038e-01, -2.6742e-01,\n",
       "         -9.2540e-01,  2.1752e-01, -5.8359e-01]], grad_fn=<SqueezeBackward1>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a368e6051e4badb14f2e71121f4ce0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647051297e744af08a4318a290317653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4eee83c83af4ef68d3ba43db1128a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847360e993684fceb396c91093857c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a\n",
    "question answering dataset is the SQuAD dataset, which is entirely based on that task. If you would like to fine-tune\n",
    "a model on a SQuAD task, you may leverage the `run_squad.py`.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.6222434043884277, 'start': 34, 'end': 95, 'answer': 'the task of extracting an answer from a text given a question'}\n"
     ]
    }
   ],
   "source": [
    "out = nlp(question=\"What is extractive question answering?\", context=context)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.pipelines.question_answering.QuestionAnsweringPipeline object at 0x7fb82e1b7b20>\n"
     ]
    }
   ],
   "source": [
    "print(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
