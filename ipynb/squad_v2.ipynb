{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    \n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    \n",
    "    return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers = read_squad(\"squad/train-v2.0.json\")\n",
    "valid_contexts, valid_questions, valid_answers = read_squad(\"squad/dev-v2.0.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two -- fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_end_idx(train_answers, train_contexts)\n",
    "add_end_idx(valid_answers, valid_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0]\n",
    "# Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n",
    "# truncation=True, padding=True -> padding to max_length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizers.Encoding"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_encodings[0])\n",
    "# tokenizers.Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_encodings)\n",
    "# transformers.tokenization_utils_base.BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "        # When using huggingface's Fast Tokenizers, we can use the built-in `char_to_token()` method\n",
    "\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    # currently, Encoding has attributes of [ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing]\n",
    "    # now, it adds start_positions and end_positions in addition to the attributes above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(valid_encodings, valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=512, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SquadDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # return as a dict of [input_ids, attention_mask, start_positions, end_positions]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SquadDataset(train_encodings)\n",
    "valid_dataset = SquadDataset(valid_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'start_positions', 'end_positions'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids torch.Size([512])\n",
      "attention_mask torch.Size([512])\n",
      "start_positions torch.Size([])\n",
      "end_positions torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "for key, value in train_dataset[0].items():\n",
    "    print(key, value.shape)\n",
    "    \n",
    "    # input_ids torch.Size([512])\n",
    "    # attention_mask torch.Size([512])\n",
    "    # start_positions torch.Size([])\n",
    "    # end_positions torch.Size([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, DistilBertForQuestionAnswering\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CustomHeadRNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomHeadRNN, self).__init__()\n",
    "        \n",
    "        self.p1_lstm = nn.LSTM(config.hidden_size, config.hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.p1_drop = nn.Dropout(p=config.qa_dropout)\n",
    "        self.p1_fc   = nn.Linear(2*config.hidden_size, 1)\n",
    "\n",
    "        self.p2_lstm = nn.LSTM(config.hidden_size, config.hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.p2_drop = nn.Dropout(p=config.qa_dropout)\n",
    "        self.p2_fc   = nn.Linear(2*config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        p1_out, (h_p1, c_p1) = self.p1_lstm(hidden_states)\n",
    "        p1_out = self.p1_drop(p1_out)\n",
    "        p1_out = self.p1_fc(p1_out)\n",
    "\n",
    "        p2_out, (h_p2, c_p2) = self.p2_lstm(hidden_states, (h_p1, c_p1))\n",
    "        p2_out = self.p2_drop(p2_out)\n",
    "        p2_out = self.p2_fc(p2_out)\n",
    "\n",
    "        return torch.cat([p1_out, p2_out], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.qa_outputs = CustomHeadRNN(config)\n",
    "# insert custom-defined head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = train_dataset[0]\n",
    "\n",
    "input_ids = example['input_ids'].unsqueeze(0)\n",
    "attention_mask = example['attention_mask'].unsqueeze(0)\n",
    "\n",
    "output = model.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "last_hidden_state = output['last_hidden_state']\n",
    "\n",
    "last_hidden_state.shape\n",
    "# [batch_size, model_max_length, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_output = model.qa_outputs(last_hidden_state)\n",
    "qa_output.shape\n",
    "# [batch_size, model_max_length, positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForQuestionAnswering(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): CustomHeadRNN(\n",
       "    (p1_lstm): LSTM(768, 768, batch_first=True, bidirectional=True)\n",
       "    (p1_drop): Dropout(p=0.1, inplace=False)\n",
       "    (p1_fc): Linear(in_features=1536, out_features=1, bias=True)\n",
       "    (p2_lstm): LSTM(768, 768, batch_first=True, bidirectional=True)\n",
       "    (p2_drop): Dropout(p=0.1, inplace=False)\n",
       "    (p2_fc): Linear(in_features=1536, out_features=1, bias=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import EvalPrediction\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad\")\n",
    "\n",
    "def compute_metrics(pred: EvalPrediction):\n",
    "    print(pred.predictions)\n",
    "    print(pred.label_ids)\n",
    "    return metric.compute(predictions=pred.predictions, references=pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_exact_match(start_positions, end_positions, start_logits, end_logits):\n",
    "    start_pred = torch.argmax(start_logits)\n",
    "    end_pred = torch.argmax(end_logits)\n",
    "\n",
    "    start_match = start_pred == start_positions\n",
    "    end_match = end_pred == end_positions\n",
    "    exact_match = torch.all(torch.cat([start_match.unsqueeze(1), end_match.unsqueeze(1)], dim=1), dim=1)\n",
    "\n",
    "    return torch.mean(exact_match.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== epoch 0 ==========\n",
      "train_loss: 1.541540998464311\n",
      "train_ems: 0.0029372465904902324\n",
      "\n",
      "valid_loss: 1.2990654921394182\n",
      "valid_ems: 0.0284897476340694\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-64615dca9e1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mem_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_exact_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 이 셀은 실행 안해도 됩니다!\n",
    "# 아래에 있는 Trainer를 실행해주세요!\n",
    "\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "train_ems = []\n",
    "val_losses = []\n",
    "val_ems = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for idx, batch in enumerate(train_loader):\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids, \n",
    "            attention_mask=attention_mask, \n",
    "            start_positions=start_positions,\n",
    "            end_positions=end_positions,\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        start_logits = outputs.start_logits\n",
    "        end_logits = outputs.end_logits\n",
    "        em_score = compute_exact_match(start_positions, end_positions, start_logits, end_logits)\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_ems.append(em_score.item())\n",
    "\n",
    "    print(\"=\"*10 + \" epoch {} \".format(epoch) + \"=\"*10)\n",
    "    print(\"train_loss:\", np.mean(train_losses[-idx:]))\n",
    "    print(\"train_ems:\", np.mean(train_ems[-idx:]))\n",
    "    print()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(val_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            start_positions = batch['start_positions'].to(device)\n",
    "            end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids, \n",
    "                attention_mask=attention_mask, \n",
    "                start_positions=start_positions,\n",
    "                end_positions=end_positions,\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            start_logits = outputs.start_logits\n",
    "            end_logits = outputs.end_logits\n",
    "\n",
    "            em_score = compute_exact_match(start_positions, end_positions, start_logits, end_logits)\n",
    "\n",
    "            val_losses.append(loss.item())\n",
    "            val_ems.append(em_score.item())\n",
    "\n",
    "    print(\"valid_loss:\", np.mean(val_losses[-idx:]))\n",
    "    print(\"valid_ems:\", np.mean(val_ems[-idx:]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 마찬가지로 이 셀은 실행 안해도 됩니다!\n",
    "# 아래에 있는 Trainer를 실행해주세요!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "ax[0].plot(train_losses)\n",
    "ax[0].plot(val_losses)\n",
    "\n",
    "ax[1].plot(train_ems)\n",
    "ax[1].plot(val_ems)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./saved',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    do_train=True,\n",
    "    do_eval=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthis-is-real\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/this-is-real/huggingface/runs/1m5arkvu\" target=\"_blank\">./saved</a></strong> to <a href=\"https://wandb.ai/this-is-real/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='621' max='16281' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  621/16281 05:12 < 2:11:46, 1.98 it/s, Epoch 0.11/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.868300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.551600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>3.553400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.847900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.485300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.094700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
