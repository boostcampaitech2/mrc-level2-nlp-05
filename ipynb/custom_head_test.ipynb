{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Custom Head Layer Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from importlib import import_module\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    DataCollatorWithPadding,\n",
    "    AdamW,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser\n",
    ")\n",
    "\n",
    "from datasets import load_from_disk, load_metric\n",
    "\n",
    "from preprocessor import BaselinePreprocessor\n",
    "from postprocessor import post_processing_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Tokenizer, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = 'klue/roberta-large'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.dropout_ratio = 0.5\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name, config=config)\n",
    "optimizer = AdamW(params=model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e053032504843458b7dcf9c14bad2be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b957dad5b1ad4892afb8eb70bc426de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1723935c474c298adbf796cfd9c8a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "question_column_name = \"question\"\n",
    "context_column_name = \"context\"\n",
    "answer_column_name = \"answers\"\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # truncation과 padding(length가 짧을때만)을 통해 toknization을 진행하며, stride를 이용하여 overflow를 유지합니다.\n",
    "    # 각 example들은 이전의 context와 조금씩 겹치게됩니다.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_token_type_ids=False, # roberta모델을 사용할 경우 False, bert를 사용할 경우 True로 표기해야합니다.\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # 길이가 긴 context가 등장할 경우 truncate를 진행해야하므로, 해당 데이터셋을 찾을 수 있도록 mapping 가능한 값이 필요합니다.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # token의 캐릭터 단위 position를 찾을 수 있도록 offset mapping을 사용합니다.\n",
    "    # start_positions과 end_positions을 찾는데 도움을 줄 수 있습니다.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 데이터셋에 \"start position\", \"enc position\" label을 부여합니다.\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)  # cls index\n",
    "\n",
    "        # sequence id를 설정합니다 (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # 하나의 example이 여러개의 span을 가질 수 있습니다.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "\n",
    "        # answer가 없을 경우 cls_index를 answer로 설정합니다(== example에서 정답이 없는 경우 존재할 수 있음).\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # text에서 정답의 Start/end character index\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # text에서 current span의 Start token index\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # text에서 current span의 End token index\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # 정답이 span을 벗어났는지 확인합니다(정답이 없는 경우 CLS index로 label되어있음).\n",
    "            if not (\n",
    "                offsets[token_start_index][0] <= start_char\n",
    "                and offsets[token_end_index][1] >= end_char\n",
    "            ):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # token_start_index 및 token_end_index를 answer의 끝으로 이동합니다.\n",
    "                # Note: answer가 마지막 단어인 경우 last offset을 따라갈 수 있습니다(edge case).\n",
    "                while (\n",
    "                    token_start_index < len(offsets)\n",
    "                    and offsets[token_start_index][0] <= start_char\n",
    "                ):\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "def prepare_validation_features(examples):\n",
    "    # truncation과 padding(length가 짧을때만)을 통해 toknization을 진행하며, stride를 이용하여 overflow를 유지합니다.\n",
    "    # 각 example들은 이전의 context와 조금씩 겹치게됩니다.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        return_token_type_ids=False, # roberta모델을 사용할 경우 False, bert를 사용할 경우 True로 표기해야합니다.\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    # 길이가 긴 context가 등장할 경우 truncate를 진행해야하므로, 해당 데이터셋을 찾을 수 있도록 mapping 가능한 값이 필요합니다.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # evaluation을 위해, prediction을 context의 substring으로 변환해야합니다.\n",
    "    # corresponding example_id를 유지하고 offset mappings을 저장해야합니다.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # sequence id를 설정합니다 (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "        # 하나의 example이 여러개의 span을 가질 수 있습니다.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "        # Set to None the offset_mapping을 None으로 설정해서 token position이 context의 일부인지 쉽게 판별 할 수 있습니다.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "    return tokenized_examples\n",
    "\n",
    "datasets = load_from_disk('/opt/ml/data/train_dataset')\n",
    "train_dataset = datasets['train']\n",
    "eval_dataset = datasets['validation'] \n",
    "eval_dataset_for_predict = datasets['validation']\n",
    "\n",
    "column_names = train_dataset.column_names\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "eval_dataset = eval_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "eval_dataset_for_predict = eval_dataset_for_predict.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'end_positions', 'input_ids', 'start_positions'],\n",
       "    num_rows: 7978\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'end_positions', 'input_ids', 'start_positions'],\n",
       "    num_rows: 474\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'example_id', 'input_ids', 'offset_mapping'],\n",
       "    num_rows: 474\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset_for_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    collate_fn = data_collator,\n",
    "    batch_size=4\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    collate_fn = data_collator,\n",
    "    batch_size=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=2, bias=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name, config=config)\n",
    "model.qa_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Custom Head Class Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 파라미터 갯수 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base total params : 2,050\n",
      "cnn total params : 3,150,850\n"
     ]
    }
   ],
   "source": [
    "class CustomHeadBase(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomHeadBase, self).__init__()\n",
    "        self.fc = nn.Linear(config.hidden_size, config.num_labels)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        return self.fc(hidden_states)\n",
    "\n",
    "class CustomHeadCNN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomHeadCNN, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv_1 = nn.Conv1d(\n",
    "            in_channels=config.hidden_size, \n",
    "            out_channels=config.hidden_size // 3,\n",
    "            kernel_size=1, \n",
    "            padding=0)  # stride: default 1\n",
    "        self.conv_3 = nn.Conv1d(\n",
    "            in_channels=config.hidden_size, \n",
    "            out_channels=config.hidden_size // 3, \n",
    "            kernel_size=3, \n",
    "            padding=1)\n",
    "        self.conv_5 = nn.Conv1d(\n",
    "            in_channels=config.hidden_size, \n",
    "            out_channels=config.hidden_size // 3 + 1,  # concat 합칠 때 맞아 떨어지도록\n",
    "            kernel_size=5, \n",
    "            padding=2)\n",
    "        self.fc = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        conv1_out = self.relu(self.conv_1(x).transpose(1, 2).contiguous())\n",
    "        conv3_out = self.relu(self.conv_3(x).transpose(1, 2).contiguous())\n",
    "        conv5_out = self.relu(self.conv_5(x).transpose(1, 2).contiguous())\n",
    "        output = self.fc(torch.cat((conv1_out, conv3_out, conv5_out), -1))\n",
    "        return output\n",
    "\n",
    "\n",
    "# qa_outputs 변경\n",
    "custom_head = CustomHeadBase(config)\n",
    "model.qa_outputs = custom_head\n",
    "\n",
    "total_params = 0\n",
    "for name, param in model.qa_outputs.named_parameters():\n",
    "    flat = torch.flatten(param)\n",
    "    total_params += flat.shape[0]\n",
    "\n",
    "print(f\"base total params : {total_params:,d}\")\n",
    "\n",
    "#############################################################\n",
    "\n",
    "custom_head = CustomHeadCNN(config)\n",
    "model.qa_outputs = custom_head\n",
    "\n",
    "total_params = 0\n",
    "for name, param in model.qa_outputs.named_parameters():\n",
    "    flat = torch.flatten(param)\n",
    "    total_params += flat.shape[0]\n",
    "\n",
    "print(f\"cnn total params : {total_params:,d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 384, 2])\n"
     ]
    }
   ],
   "source": [
    "class CustomHeadCNNWithMaxPool(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(CustomHeadCNNWithMaxPool, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv_1 = nn.Conv1d(\n",
    "            in_channels=config.hidden_size, \n",
    "            out_channels=config.hidden_size // 3,\n",
    "            kernel_size=1, \n",
    "            padding=0)  # stride: default 1\n",
    "        self.conv_3 = nn.Conv1d(\n",
    "            in_channels=config.hidden_size, \n",
    "            out_channels=config.hidden_size // 3, \n",
    "            kernel_size=3, \n",
    "            padding=1)\n",
    "        self.conv_5 = nn.Conv1d(\n",
    "            in_channels=config.hidden_size, \n",
    "            out_channels=config.hidden_size // 3 + 1,  # concat 합칠 때 맞아 떨어지도록\n",
    "            kernel_size=5, \n",
    "            padding=2)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=1)\n",
    "        self.dropout = nn.Dropout(p=config.dropout_ratio)\n",
    "        self.fc = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        self.sample = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out_sample = self.sample(x)\n",
    "        x = x.transpose(1,2).contiguous()\n",
    "        \n",
    "        conv1_out = self.maxpool(self.relu(self.conv_1(x).transpose(1, 2).contiguous()))\n",
    "        conv3_out = self.maxpool(self.relu(self.conv_3(x).transpose(1, 2).contiguous()))\n",
    "        conv5_out = self.maxpool(self.relu(self.conv_5(x).transpose(1, 2).contiguous()))\n",
    "\n",
    "        output = self.fc(self.dropout(torch.cat((conv1_out, conv3_out, conv5_out), -1))) # concat, dropout\n",
    "        \n",
    "        return out_sample\n",
    "\n",
    "\n",
    "# qa_outputs 변경\n",
    "config.dropout_ratio = 0.5\n",
    "custom_head = CustomHeadCNNWithMaxPool(config)\n",
    "model.qa_outputs = custom_head\n",
    "\n",
    "# layer별 출력 크기 확인\n",
    "for batch in train_dataloader:\n",
    "    model.train()\n",
    "    outputs = model(**batch)\n",
    "    #print(outputs.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
