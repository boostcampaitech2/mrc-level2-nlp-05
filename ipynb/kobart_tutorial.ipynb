{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kobart\n",
      "  Cloning https://github.com/SKT-AI/KoBART to /tmp/pip-install-ja2kghcd/kobart_6afcd913a3914e4289c8411c7f32a54e\n",
      "  Running command git clone --filter=blob:none -q https://github.com/SKT-AI/KoBART /tmp/pip-install-ja2kghcd/kobart_6afcd913a3914e4289c8411c7f32a54e\n",
      "  Resolved https://github.com/SKT-AI/KoBART to commit 0636edf20390cbc72e8f676c6e8aeee086fd4f95\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers==4.3.3 in /opt/conda/lib/python3.8/site-packages (from kobart) (4.3.3)\n",
      "Requirement already satisfied: torch==1.7.1 in /opt/conda/lib/python3.8/site-packages (from kobart) (1.7.1)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.7.1->kobart) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torch==1.7.1->kobart) (1.19.2)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (0.0.46)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (0.10.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (21.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (2.24.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (2021.10.8)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers==4.3.3->kobart) (4.62.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->transformers==4.3.3->kobart) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.3.3->kobart) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.3.3->kobart) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.3.3->kobart) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers==4.3.3->kobart) (2021.10.8)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.3.3->kobart) (1.15.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.3.3->kobart) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers==4.3.3->kobart) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/SKT-AI/KoBART#egg=kobart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from transformers import BartModel, BartForConditionalGeneration\n",
    "from kobart import get_pytorch_kobart_model, get_kobart_tokenizer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_kobart_tokenizer()\n",
    "model = BartForConditionalGeneration.from_pretrained(get_pytorch_kobart_model())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='', vocab_size=30000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'mask_token': '<mask>'})\n",
      "BartForConditionalGeneration(\n",
      "  (model): BartModel(\n",
      "    (shared): Embedding(30000, 768, padding_idx=3)\n",
      "    (encoder): BartEncoder(\n",
      "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768, padding_idx=3)\n",
      "      (layers): ModuleList(\n",
      "        (0): BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1028, 768, padding_idx=3)\n",
      "      (layers): ModuleList(\n",
      "        (0): BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', '</s>', '<usr>', '<pad>', '<sys>', '<unk>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens([0, 1, 2, 3, 4, 5,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = '''한국 전쟁 직후 현대건설은 전쟁으로 파괴된 도시와 교량, 도로, 집, 건물 등을 복구하면서 점차 늘어가는 건설수요로 승승장구하게 되었다. 그 뒤에도 늘어나는 건설 수요 등을 감안하여 그는 시멘트 공장 설립을 추진, 1964년 6월 현대 시멘트공장을 준공하여 시멘트도 자체적으로 조달하였다. 그 뒤 낙동강 고령교 복구, 한강 인도교 복구, 제1한강교 복구, 인천 제1도크 복구 등의 사업을 수주하여 1960년에는 국내 건설업체중 도급한도액이 1위를 차지하게 되었다. 1964년 단양에 시멘트 공장을 완공하였으며, 1965년에는 국내 최초로 태국의 파타니 나라티왓 고속도로를 건설하였다. 1967년에는 다시 자동차 산업에 뛰어들어 현대자동차주식회사를 설립하였다.\\n\\n현대건설 내 시멘트공장을 확장하여 1970년 1월 정식으로 현대시멘트주식회사를 설립하였다. 이후 현대건설과 현대시멘트의 사주로 해외건설시장 확보와 낙찰 등을 이끌어내며 한국 국외의 건설시장으로도 진출하였고 울산 조선소 건설, 서산 앞바다 간척사업 등을 성공적으로 추진하면서 기업을 확장하게 된다.\\n\\n1971년 1월 현대자동차, 현대건설, 현대시멘트주식회사 등을 총괄한 현대그룹을 창립하고 대표이사 회장에 취임하였다. 1973년 12월에는 중공업에도 진출하였다.\n",
    "'''\n",
    "inputs = tokenizer([test_text], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartEncoder(\n",
      "  (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
      "  (embed_positions): BartLearnedPositionalEmbedding(1028, 768, padding_idx=3)\n",
      "  (layers): ModuleList(\n",
      "    (0): BartEncoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): BartEncoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): BartEncoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): BartEncoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): BartEncoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): BartEncoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = model.get_encoder()\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 260, 768])\n"
     ]
    }
   ],
   "source": [
    "out = encoder(inputs['input_ids'])\n",
    "print(out[0].shape)\n",
    "# 1, 261, 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartDecoder(\n",
      "  (embed_tokens): Embedding(30000, 768, padding_idx=3)\n",
      "  (embed_positions): BartLearnedPositionalEmbedding(1028, 768, padding_idx=3)\n",
      "  (layers): ModuleList(\n",
      "    (0): BartDecoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (1): BartDecoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (2): BartDecoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (3): BartDecoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (4): BartDecoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (5): BartDecoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "      (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = model.get_decoder()\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'한국 전쟁 직후 현대건설은 전쟁으로 파괴된 도시와 교량, 도로, 집, 도로, 집, 건물 등을 복구하면서 점차 늘어가는 건설 수요로 승승장구하게 되었하였다. 그 뒤에도 늘어나는 건설 수요 등을 감안하여 그는 시멘트 공장 설립을 추진, 1964년 6월 현대 시멘트공장을 준공하여 시멘트도 자체적으로 조달하였다. 그 뒤 낙동강 고령교 복구, 한강 인도교 복구, 한강 인도교 복구, 제1한강교 복구, 제1한강교 복구, 인천 제1도크 복구 등의 사업을 수주하여 1960년에는 국내 건설업체중 도급도크 복구 등의 사업을 수주하여 1960년에는 국내 건설업체중 도급한도액이 1위를 차지하게 되었다. 1964년 단양에 시멘트 공장을 완공하였으며, 1965년에는 국내 최초로'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_out = model.generate(inputs['input_ids'], num_beams=16, min_length=30, max_length=160, early_stopping=True)\n",
    "tokenizer.decode(generate_out[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
