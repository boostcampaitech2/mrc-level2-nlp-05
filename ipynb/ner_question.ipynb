{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pororo의 NER Tagger를 이용한 Data Augmentation\n",
    "\n",
    "### PORORO: Platform Of neuRal mOdels for natuRal language prOcessing [Github link](https://github.com/kakaobrain/pororo)\n",
    "\n",
    "\n",
    "## 목표: 데이터 중 질문(\"question\") 문장에 NER 태그를 추가하는 data augmentation을 수행합니다.\n",
    "\n",
    "예시) '해마다 식중독으로 목숨을 잃는 인구는?' -> '해마다 질병 식중독으로 목숨을 잃는 인구는?'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛠 가상환경 세팅\n",
    "\n",
    "```\n",
    "conda create -n pororo_env python=3.8\n",
    "conda activate pororo_env\n",
    "pip install pororo\n",
    "pip install datasets==1.5.0  # 중요!!\n",
    "pip install jupyter\n",
    "# VSCode 종료 후 재실행\n",
    "# 우측 상단 select kernel 클릭 후 'pororo_env':conda 선택\n",
    "# 노트북 실행\n",
    "conda deactivate # 실험 완료 시\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pororo import Pororo\n",
    "from datasets import load_from_disk\n",
    "from typing import List, Tuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = Pororo(task=\"ner\", lang=\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pororo ner task에서 쓰인 전체 태그 목록을 번역하여 매핑함\n",
    "ENG_TO_KOR = {'PERSON' : '사람', 'LOCATION' : '장소', 'ORGANIZATION' : '조직', 'ARTIFACT' : '인공물',\n",
    "            'DATE' : '날짜', 'TIME' : '시간', 'CIVILIZATION' : '역할', 'ANIMAL' : '동물',\n",
    "            'PLANT' : '식물', 'QUANTITY' : '수량', 'STUDY_FIELD' : '분야', 'THEORY' : '이론',\n",
    "            'EVENT' : '사건', 'MATERIAL' : '물질', 'TERM' : '용어', \n",
    "            'OCCUPATION' : '직업', 'COUNTRY' : '국가', 'CITY' : '도시', 'DISEASE' :'질병'}\n",
    "\n",
    "# 'O': 태그가 없는 경우\n",
    "# 직접 지정한 부자연스러운 태그 집합 (아래 설명)\n",
    "IGNORE_TAGS = ('O', 'ARTIFACT', 'DATE', 'TIME', 'CIVILIZATION', 'QUANTITY', 'TERM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 증강된 데이터를 눈으로 확인해본 결과, 부자연스러운 태그가 많이 발견되었습니다. 따라서 `'ARTIFACT', 'DATE', 'TIME', 'CIVILIZATION', 'QUANTITY', 'TERM'` 총 6가지 태그는 제외하고 진행했습니다. 부자연스러운 문장의 예시는 아래와 같습니다.\n",
    "\n",
    "> ARTIFACT: 인공물\n",
    "\n",
    "'노선이 확대되기 전 인공물 열차의 객차는 몇 개였나?',\n",
    "\n",
    "'인공물 아다케부네가 제작 된 시대는?',\n",
    "\n",
    " '《인공물 가우스전자》의 원래 제목은 무엇이었나?',\n",
    " \n",
    "\n",
    "> DATE: 날짜\n",
    "\n",
    " '날짜 고려시대 때 마을의 걱정을 없애기 위해 만든 석불은 무엇으로 만들어졌나요?',\n",
    "\n",
    " '국가 스웨덴과 함께 날짜 겨울 전쟁이 끝나길 바란 국가는?',\n",
    "\n",
    " '이론 풍수지리사상의 관점에서 보면 날짜 선몽대는 어떤 유형에 속하나요?',\n",
    "\n",
    "\n",
    "> TIME: 시간\n",
    "\n",
    " '사람 게바라가 7월 23일 시간 오후에 간 곳은 어디인가?',\n",
    "\n",
    " '비가 내린 뒤 시간 밤에 결혼비행을 하는 개체는 무엇인가?',\n",
    "\n",
    " '시간 90분이 종료된 이후 더 주어진 시간동안 몇 골을 성공시켰는가?',\n",
    "\n",
    "\n",
    "> CIVILIZATION: 역할\n",
    "\n",
    " '정치하는 역할 엄마들이 문명 피해자들이 정상적인 일상생활을 할 수 있도록 돕기 위해 진행했던 서비스는?',\n",
    "\n",
    " '역할 수령자가 매매계약을 파기하기 위해서 지불해야하는 것은?',\n",
    "\n",
    " '사람 코페르니쿠스가 역할 교회법 박사 학위를 받은 날은?',\n",
    " \n",
    "> QUANTITY: 수량\n",
    "\n",
    " '사람 카이가 수량 22살에 한 일은?',\n",
    "\n",
    " '수량 제 1차 사건 바르바리 전쟁에서 지휘를 맡았던 사람은?',\n",
    "\n",
    " '사람 허계임과 그녀의 수량 두 딸 중 가장 먼저 순교한 것은 누구인가?',\n",
    "\n",
    "> TERM: 용어\n",
    "\n",
    " '용어 뇌에 탈피 신호를 보내는 부위는?',\n",
    "\n",
    " '완장에 있는 용어 별모양과 비슷한 모양의 빛을 뿜는 것은 누구의 동물 손인가?',\n",
    " \n",
    " '전기화학적 기울기가 용어 ATP와 무엇을 만들어낼 수 있는가?',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_aug(\n",
    "    word_tag_pairs: List[Tuple[str, str]],\n",
    "    valid_tag: List[bool],\n",
    "    max_tag_num: int = 2) -> str:\n",
    "    '''\n",
    "    문장에 NER 태그를 random하게 추가해주는 함수.\n",
    "\n",
    "    Args:\n",
    "        word_tag_pairs: 입력 문장의 단어와 NER 태그 쌍을 담은 리스트\n",
    "        valid_tag: 의미 있는 태그를 갖는지에 대한 여부를 나타내는 리스트\n",
    "        max_tag_num: 출력 문장에 추가할 태그의 최대 개수\n",
    "    Returns:\n",
    "        aug_sentence: NER 태그가 단어 앞에 추가된 문장\n",
    "    \"\"\"\n",
    "    '''\n",
    "\n",
    "    tag_num = sum(valid_tag)  # total num of tags\n",
    "    \n",
    "    if tag_num > max_tag_num:\n",
    "        valid_tag = np.array(valid_tag)\n",
    "        tag_pos_idx = np.where(valid_tag)\n",
    "\n",
    "        # leave only max_tag_num positions to True\n",
    "        set_false_pos = np.random.choice(tag_pos_idx[0], tag_num - max_tag_num, replace=False)\n",
    "        valid_tag[set_false_pos] = False\n",
    "    \n",
    "    aug_sentence = ''\n",
    "    # create a new sentence with tags inserted\n",
    "    for item, add_tag in zip(word_tag_pairs, valid_tag):\n",
    "        if add_tag and item[1] not in IGNORE_TAGS:\n",
    "            aug_sentence += f'{ENG_TO_KOR[item[1]]} '\n",
    "        aug_sentence += item[0]      \n",
    "    return aug_sentence\n",
    "\n",
    "\n",
    "def add_ner_tag(examples):\n",
    "    \"\"\"\n",
    "    데이터 증강 시 map에서 적용할 함수.\n",
    "    batch 데이터를 받으면 \"question\" 문장들을 변형시켜 batch 데이터를 반환함.\n",
    "    \"\"\"\n",
    "    new_questions = []\n",
    "    for question in examples[\"question\"]:\n",
    "        ner_question = ner(question)  # list of (word, tag)\n",
    "        tag_pos = [False if word[1] in IGNORE_TAGS else True for word in ner_question]\n",
    "        new_questions.append(random_aug(ner_question, tag_pos, max_tag_num=2))\n",
    "    return {\"question\": new_questions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 증강에 쓰일 데이터셋을 load합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_from_disk(\"/opt/ml/data/train_dataset\")\n",
    "train_dataset = datasets[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `datasets.Dataset.map()`을 이용하여 batch 단위로 데이터 증강을 진행합니다.\n",
    "\n",
    "시간이 꽤 많이 소요됩니다. (-30분) 따라서 우선 sample 30개만으로 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.select(range(30))  # sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Korean Sentence Splitter]: Initializing Pynori...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a2f2abd4d54b118c8a26ec43fcdae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset_aug = train_dataset.map(\n",
    "                        add_ner_tag,\n",
    "                        batched=True,\n",
    "                        batch_size=8,\n",
    "                        # num_proc=4  # 2 이상 설정 시, Pororo NER에서 에러 발생\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `datasets.Dataset.save_to_disk()`로 저장합니다.\n",
    "`save_to_disk()`로 저장하면 `load_from_disk()`로 쉽게 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"train\"] = train_dataset_aug\n",
    "datasets.save_to_disk(\"/opt/ml/data/ner_only_train_dataset_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `datasets.Dataset.load_from_disk()`로 load까지 성공했는지 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_to_aug = load_from_disk(\"/opt/ml/data/ner_only_train_dataset_3/\")\n",
    "train_dataset_to_aug = datasets_to_aug[\"train\"]\n",
    "# type(train_dataset_to_aug)  # datasets.arrow_dataset.Dataset\n",
    "len(train_dataset_to_aug)  # 30"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5a2abfc5a0e1ec78ff24fbbc507cd419539823e64f93fb79828c29304e432d23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('pororo_env2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
