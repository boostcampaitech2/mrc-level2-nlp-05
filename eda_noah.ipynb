{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 특수문자 EDA (MRC 팀 진행)\n",
    "- Train 폴더의 train, valid set의 question, context, answer 에 포함된 특수문자\n",
    "- Answer 에는 포함되지 않지만 Context 에만 있는 특수문자에 대해서...!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set의 answers에 포함된 특수문자를 제외한 context에 포함된 특수문자:\n",
      "['!!', '!!!', '!!\\\\', '!!》:', '!\"\\\\', \"!'\", '!)', '!>', '!\\\\', '!’', '!’(', '!’\\\\', '!”', '!”\\\\', '!」', '\"\"', '\"\\'', '\"(', '\"(\"', '\"(-', '\")', '\").', '\")\\\\', '\",', '\"-', '\".', '\"...', '\"/>', '\">', '\"\\\\', '\"|', '\"~', '\"·\"', '\"é', '\"НІ\"', '\"”', '\"《', '#', '$', '%\"', '%)', '%,', '%~', '&', \"'(\", \"'('\", \"')\", \"')\\\\\", \"',\", \"'-\", \"'é\", \"'œ\", \"'》\", \"'ㅚ'\", \"'ㅟ'\", '(\"', '(\"\"', '(#', \"('\", '()', '(*', '(+)', '(-)', '(--', '(.', '(...)', '(:', '(<', '(=', '(?)', '([ˈ', '(~', '(É', '(Þ', '(Þó', '(ó', '(ə)', '(Αθηναι)', '(Εὔφορβος)', '(Κένταυροι)', '(Μήδεια)', '(Στύξ,', '(Φάλαγγα)', '(Φαίδρα)', '(εὔδοξος)', '(κατάφρακτοι)', '(κατάφρακτος),', '(φαιδρός)', '(Василия', '(Лев)', '(Народничество)', '(Ю́рий', '(княз)\"', '(козаки́|', '(нон)', '(מׇשִׁיחַ|', '(إمام,', '(الكاشغري)', '(المعتزلة\\u200e)', '(تقیة', '(حزب', '(حصروم)', '(كربغا,', '(ملك', '(சோழ', '(გონიოს', '(დავით', '(ᛚᚢᚴᛁ;', '(ὁπλίτης)', '(‘', '(“', '(…)', '(←', '(→', '(≪', '(①', '(□□□', '(〈', '(《', ')#', \")'\", \")',\", \")'.\", ')(', '))', '),', ')-', ').', ').\"', ')...', ').\\\\', ').”', ').」', '):', '):\\\\', ')<', ')=', ')>', ')>(', ')>>', ')[', ')\\\\', ')]', ')]</', ')|', ')·', ')―', ')’', ')’!', ')’,', ')“', ')…', ')Ⅰ’', ')、', ')〉', ')》(', ')」', ')』', ')・', ')ㆍ', ')＞', '*', '**', '****,', '*.', '*‘', '*①', '*②', '++', '++,', '+α', ',\"', ',)', ',\\\\', ',‘', ',“', ',”', ',《', ',》', ',『', ',ＭＳ', '-\"', '-)', '--', '---', '->', '-Þó', '-α-', '.\"', '.\",', '.\"\\\\', \".'\", \".'\\\\\", '.(', '.(*', '.(‘', '.(→', '.(《', '.)', '.)\"', '.),', '.).', '.)</', '.)\\\\', '.,', '..', '..\"', '...\"', '...\"\\\\', \"...'\", '...(', '...)\\\\', '....', '...\\\\', '...|', '..</', '..\\\\', '.:\\\\', '.<', '.</', '.[', '.\\\\', '.]', '.].', '.|', '.|《', '.·', '.¹', '.’', '.“', '.”(', '.”-', '.”\\\\', '.”|', '.”}\\\\', '.”…', '.…', '.《', '.「', '.」', '.」,', '.『', '.\\uf000', '.＜', '////＼＼＼＼＼', '/>', '/>,', '/ㅏ', '/ㅏ/', ':\"', ':(', ':*', ':-', '::', ':\\\\', ':Лира)', ':सितार,', ':সেতার,', ';', '</', '<<', '=', '=\"', '==', '===', '==\\\\', '>(', '>,', '>>', '>>,', '>\\\\', '>”', '?', '?\"', \"?'\", \"?',\", \"?'\\\\\", '?)', '?-', '?\\\\', '?“', '?”', '?”\\\\', '?〉', '?》', '?」.\\\\', '?』', '[', '[...]', '[[', '\\\\', ']', '](', '])', '],', ']-', ']</', '][', ']\\\\', ']]', \"]]')\", ']].', '^', '_', '__', '`', '`,', '{|\\\\', '{“', '|', '|[', '|}\\\\', '}}', '}})', '}}).\\\\', '}}</', '}}\\\\', '~)', '~”', '\\xad〉', '°,', '±', '²', '²(', '²)', '²,', '´´', 'µ', '·,', '···', '¾', 'É', 'Ó', '×', 'Þ', 'Þó', 'ß', 'à', 'á', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ë),', 'ì', 'í', 'ð', 'ó', 'óð)', 'ö', 'öð)', 'ù', 'ù,', 'ú', 'ü', 'ý', 'ā', 'ă|', 'ć)', 'Đạ', 'đườ', 'ē', 'ğ', 'ī', 'ł', 'ł)', 'łł', 'ū', 'ŭ', 'ơ', 'ǫ', 'ɔ', 'ɛ', 'ʲ', 'ˈ', 'ː', 'Δ΄', 'Λιμός)', 'Μεγάλος', 'Μιχαήλ', 'Πάτρα)', 'Πάτραι)', 'Παφλαγών,', 'Πόντος)', 'Στύγα,', 'α', 'α-', 'β', 'β-', 'δρακων', 'Миха́йлович', 'Пак:', 'Смирно́в)', 'каза́ки|', 'казакі)', 'о', 'повешенных)', 'семи', 'מׇשִׁחַ|', 'الإمام,', 'الله|', 'المملوك|', 'امام)', 'تار)', 'ستار,', 'سی', 'சாம்ராஜ்யம்)', 'ციხე,', 'ᐨ', 'ầ', 'ữ', 'Ἰώ)', '–', '—', '——', '―', '――', '‘□□□', '’(', '’(क़ुली)', '’)', '’,', '’=’', '’·‘', '“(', '”(', '”,', '”.', '”[', '”\\\\', '•', '․', '…', '…\"', '…(', '….', '….\\\\', '…”', '……', '…….', '…＞', '‧', '℃', '℃)', 'Ⅰ', 'Ⅰ’', 'Ⅱ', '←', '↑↓↑↓', '→', '⇌', '∼', '≫(', '①', '②', '③', '④', '⑤', '⑥', '⑦', '⑧', '⑨', '⑪', '▲', '△', '○', '☆', '☞', '♣', '、', '。', '。\\\\', '〉)', '〉,', '〉.', '〉〈’', '》(Рассказ', '》(《', '》)', '》)\\\\', '》,', '》,《', '》·《', '》《', '「~', '「「', '」(', '」,', '」.', '』(', '』,', '』,『', '』.', '【', '】\\\\', '〔', '〕', 'ヶ', 'ㅏ', 'ㅕ', 'ㆍ', '㈜', '㎜', '㎜,', '㎝', '㎝)', '㎝,', '㎞', '㎠', '㎡', '\\uf000', '（', '）', '）.', '）\\\\', '，', '，《', '－', '．', '：', '＝', '＞(', '＞,', '？\"', '？)', 'ＭＳ', 'ＲＸ', 'Ｖ', '～', '｢', '｣']\n",
      "\n",
      "validation set의 answers에 포함된 특수문자를 제외한 context에 포함된 특수문자:\n",
      "['!', '!\"', '!\"(\"', '!)', '!\\\\', '!”', '!”(', '\"(чересполосица)', '\")', '\",', '#', '$', '%', '&', \"')\\\\\", \"',\", '((‘', '(?)', '(Лев)', '(பல்லவ)', '(பல்லவநாடு|', '(“', ')\"', \")'\", '),', ').', ').\\\\', ')/', '):', ')\\\\', ')·', ')’', ')》', ')」', '*', '**', '+', ',\\\\', '-', '-α-', '-β', '.\"', '.\",', '.\"\\\\', \".'\", '.(', '.)', '.)\\\\', '.,', '...', '...\"', '.[...]', '.\\\\', '.]', '.|', '.’', '.”', '.”\\\\', '/', ':*', ':\\\\', ';', '<', '<<', '=', '==', '===', '===\\\\', '==\\\\', '>', '>,', '>>', '?', '?\"', '?\\\\', '?”', '?”,', '?”\\\\', '[', '\\\\', '|', '~', '°', '²', 'µ', '¾', 'è', 'é', 'ö', 'ü', 'β', '–', '’(', '’).', '’,', '“(', '“‘', '…', '……', 'ℓ', '∼', '≪', '≫', '、', '。', '。\\\\', '》(', '》,', '》,《', '》《', '「', '」', '』(', '・', 'ㆍ', '㎝', '㎞', '－', 'ＭＳ']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "datasets = load_from_disk(\"/opt/ml/data/train_dataset\")\n",
    "\n",
    "train_df = pd.DataFrame(datasets[\"train\"])\n",
    "val_df = pd.DataFrame(datasets[\"validation\"])\n",
    "\n",
    "# == Answers ====================================================================================\n",
    "\n",
    "def extract_answer_specials(df:pd.DataFrame):\n",
    "    answer_specials = []\n",
    "    for i, row in df.iterrows():\n",
    "        text = row.answers[\"text\"][0]\n",
    "        result = re.findall(r\"[^ㄱ-ㅎ가-힣A-Za-z\\d\\s]+\", text)\n",
    "        if len(result) > 0:\n",
    "            answer_specials.extend(result)\n",
    "    answer_specials = list(set(answer_specials))\n",
    "    return answer_specials\n",
    "\n",
    "train_answer_sepcials = extract_answer_specials(train_df)  # train set의 answers에 포함된 특수문자\n",
    "val_answer_sepcials = extract_answer_specials(val_df)  # validation set의 answers에 포함된 특수문자\n",
    "\n",
    "\n",
    "# == Context ====================================================================================\n",
    "def extract_context_specials(df:pd.DataFrame):\n",
    "    context_specials = []\n",
    "    for text in df[\"context\"]:\n",
    "        # 중국어: 一-龥 / 일본어: ぁ-ゔァ-ヴー々〆〤 / 러시아어(적용 X): \\u0400-\\u04FF\n",
    "        result = re.findall(r\"[^ㄱ-ㅎ가-힣A-Za-z\\d\\s一-龥ぁ-ゔァ-ヴー々〆〤]+\", text)\n",
    "        if len(result) > 0:\n",
    "            context_specials.extend(result)\n",
    "    context_specials = list(set(context_specials))\n",
    "    return context_specials\n",
    "\n",
    "train_context_sepcials = extract_context_specials(train_df)  # train set의 context에 포함된 특수문자\n",
    "val_context_sepcials = extract_context_specials(val_df)  # validation set의 context에 포함된 특수문자\n",
    "\n",
    "\n",
    "print(\"train set의 answers에 포함된 특수문자를 제외한 context에 포함된 특수문자:\")\n",
    "train_specials = sorted([special for special in train_context_sepcials if special not in train_answer_sepcials])\n",
    "print(train_specials)\n",
    "print()\n",
    "\n",
    "print(\"validation set의 answers에 포함된 특수문자를 제외한 context에 포함된 특수문자:\")\n",
    "val_specials = sorted([special for special in val_context_sepcials if special not in val_answer_sepcials])\n",
    "print(val_specials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(成田のようにならないようにしよう', ')\"', '(郵務學堂)', '”', '(釋總)', '(片刀)', ',', '＞', ')》', '(康平)', '(扶餘縣監)', '(有限性)', '「', '(魯陵)', '(祭壇)', \"'\", 'ä', '·', '〉(', '’(鉱害)', \"(海蓮之塔)'\", '》', '(受信)', '(北京)', '」', '〉', '‘', '(循資格式)', '〈', '(淵太祚)', '(蔣儼)', '(鄭夔弼)', '(塔身)', '°', '(流)', '>', '(', '(陳理)', '(軍毅)', '(原田甚次)', ')≫', '(公)〉', '(遷)', '(媒槪念)', '(冬牲)', '\"', '!\"', '\\xad', '(狼孟)', '!', '(華陽)', '(呂超)', '(部分割卵)\"', '(金學奎)', '/', '・', '’(前意識)', '~', '“', '(大)', ':', '(一心)’', '%', '《', '(華淸池)', '<', '’', ')', '二月河)', '(劉肥)', '』', '(日坂駅)', '(重)', '》(', '.', '＜', '.”', '-', ')”', '(愛琿城,', '(宋公祠)', '『', '≫', '(張勳)', '(南雲忠一)', '...', '‘∧’', '(文藏臺)', '+', '(麻辣)', '≪', '(趙匡)', '(優塡王)', '(興輪寺)', '(汪)']\n",
      "['.', '＜', '\"(', '》', \"'(\", '”', \"(陳田)'\", ',', '『', '＞', '“', '(齊西之戰)', ':', '〉', '‘', '《', '〈', '’', '(楊黨)', ')', 'ć)', '(', '(黑色肉)', '(呂正鉉)', \"'\", '』', '\"', '(物的成果)', '·']\n"
     ]
    }
   ],
   "source": [
    "print(train_answer_sepcials)\n",
    "print(val_answer_sepcials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '-', '!', '?-', '》', '/', '”', '#', '<<', '(حصروم)', '?', ',', '『', '」', '~', '“', '≫', ':', '%', '〉', '‘', '《', '〈', '<', '>>', ')', '≪', '>', '(', '「', ')>', \"'\", '』', '\"', '·'] \n",
      "\n",
      " ['?', ',', '.', '~', \"'\", '\"', '》', '%', '>', '?\"', '《', '<']\n"
     ]
    }
   ],
   "source": [
    "def extract_question_specials(df:pd.DataFrame):\n",
    "    question_specials = []\n",
    "    for text in df[\"question\"]:\n",
    "        # 중국어: 一-龥 / 일본어: ぁ-ゔァ-ヴー々〆〤 / 러시아어(적용 X): \\u0400-\\u04FF\n",
    "        result = re.findall(r\"[^ㄱ-ㅎ가-힣A-Za-z\\d\\s一-龥ぁ-ゔァ-ヴー々〆〤]+\", text)\n",
    "        if len(result) > 0:\n",
    "            question_specials.extend(result)\n",
    "    question_specials = list(set(question_specials))\n",
    "    return question_specials\n",
    "\n",
    "\n",
    "train_question_sepcials = extract_question_specials(train_df)\n",
    "val_question_sepcials = extract_question_specials(val_df)\n",
    "\n",
    "print(train_question_sepcials, '\\n'*2, val_question_sepcials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단순하게는 answer 에는 포함되어 있는 특수문자 제거 가능...!\n",
    "- 이는 학습이나 추론 단계에서 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Passage Retriever 의 Tokenizer 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='bert-base-multilingual-cased', vocab_size=119547, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}) \n",
      "\n",
      " PreTrainedTokenizerFast(name_or_path='klue/bert-base', vocab_size=32000, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer1,'\\n'*2 ,tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/wikipedia_documents.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "contexts = list(dict.fromkeys([v[\"text\"] for v in wiki.values()])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1337 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1133 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "## get seq lengths after tokenization\n",
    "seq_lengths_tokenizer1 = [len(tokenizer1(c)['input_ids']) for c in contexts]\n",
    "seq_lengths_tokenizer2 = [len(tokenizer2(c)['input_ids']) for c in contexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def show_dist(seq_len):\n",
    "    total = len(seq_len)\n",
    "    over = sum([1 for i in seq_len if i > 512])\n",
    "    over1000 = sum([1 for i in seq_len if i > 1000])\n",
    "    over1300 = sum([1 for i in seq_len if i > 1300])\n",
    "    over1600 = sum([1 for i in seq_len if i > 1600])\n",
    "    min,max = np.min(seq_len),np.max(seq_len)\n",
    "\n",
    "    print(f'Percentage over 512 tokens: {100*over/total:.2f}%')\n",
    "    print(f'Percentage over 1000 tokens: {100*over1000/total:.2f}%')\n",
    "    print(f'Percentage over 1300 tokens: {100*over1300/total:.2f}%')\n",
    "    print(f'Percentage over 1600 tokens: {100*over1600/total:.2f}%')\n",
    "    print(f'Minimum: {min}, Maximum: {max}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***For bert-base-multilingual-cased tokenizer***\n",
      "Percentage over 512 tokens: 25.35%\n",
      "Percentage over 1000 tokens: 5.54%\n",
      "Percentage over 1300 tokens: 2.75%\n",
      "Percentage over 1600 tokens: 1.55%\n",
      "Minimum: 74, Maximum: 29113\n",
      "____________________________________________________________________________________________________\n",
      "***For klue/bert-base tokenizer***\n",
      "Percentage over 512 tokens: 19.89%\n",
      "Percentage over 1000 tokens: 4.16%\n",
      "Percentage over 1300 tokens: 2.03%\n",
      "Percentage over 1600 tokens: 1.13%\n",
      "Minimum: 56, Maximum: 27541\n"
     ]
    }
   ],
   "source": [
    "# For wiki.contexts\n",
    "print(f'***For bert-base-multilingual-cased tokenizer***')\n",
    "show_dist(seq_lengths_tokenizer1)\n",
    "\n",
    "print('_'*100)\n",
    "\n",
    "print(f'***For klue/bert-base tokenizer***')\n",
    "show_dist(seq_lengths_tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29113, 25834, 17534, 16920, 16284, 12380, 12116, 10834, 10721, 9567]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(seq_lengths_tokenizer1, reverse = True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3952\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "dataset = load_from_disk(\"../data/train_dataset\")\n",
    "df = dataset['train']['context']\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get seq lengths after tokenization\n",
    "seq_lengths_tokenizer1_ = [len(tokenizer1(c)['input_ids']) for c in df]\n",
    "seq_lengths_tokenizer2_ = [len(tokenizer2(c)['input_ids']) for c in df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***For bert-base-multilingual-cased tokenizer***\n",
      "Percentage over 512 tokens: 49.67%\n",
      "Percentage over 1000 tokens: 6.48%\n",
      "Percentage over 1300 tokens: 0.20%\n",
      "Percentage over 1600 tokens: 0.00%\n",
      "Minimum: 295, Maximum: 1336\n",
      "____________________________________________________________________________________________________\n",
      "***For klue/bert-base tokenizer***\n",
      "Percentage over 512 tokens: 35.86%\n",
      "Percentage over 1000 tokens: 2.68%\n",
      "Percentage over 1300 tokens: 0.00%\n",
      "Percentage over 1600 tokens: 0.00%\n",
      "Minimum: 241, Maximum: 1174\n"
     ]
    }
   ],
   "source": [
    "# For train contexts\n",
    "\n",
    "print(f'***For bert-base-multilingual-cased tokenizer***')\n",
    "show_dist(seq_lengths_tokenizer1_)\n",
    "\n",
    "print('_'*100)\n",
    "\n",
    "print(f'***For klue/bert-base tokenizer***')\n",
    "show_dist(seq_lengths_tokenizer2_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해결방안...?\n",
    "1. Cutoff 기준으로 가운데 512?\n",
    "2. Cutoff 기준으로 random 한 starting point 설정 후 512개 사용..!\n",
    "3. 중요 문장 혹은 paraphrase 한 passage를 사용하면 안될까...? -> ex> pororo나 mecab morph 활용\n",
    "3. 아니면 일단 접고 성능 더 좋은 Sparse Retrieval 혹은 Elastic Search 기반의 서칭 방법 구축 돕기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***For bert-base-multilingual-cased tokenizer***\n",
      "Percentage over 512 tokens: 0.00%\n",
      "Percentage over 1000 tokens: 0.00%\n",
      "Percentage over 1300 tokens: 0.00%\n",
      "Percentage over 1600 tokens: 0.00%\n",
      "Minimum: 8, Maximum: 52\n",
      "____________________________________________________________________________________________________\n",
      "***For klue/bert-base tokenizer***\n",
      "Percentage over 512 tokens: 0.00%\n",
      "Percentage over 1000 tokens: 0.00%\n",
      "Percentage over 1300 tokens: 0.00%\n",
      "Percentage over 1600 tokens: 0.00%\n",
      "Minimum: 7, Maximum: 45\n"
     ]
    }
   ],
   "source": [
    "df= dataset['train']['question']\n",
    "\n",
    "## get seq lengths for questions after tokenization\n",
    "seq_lengths_tokenizer1_q = [len(tokenizer1(c)['input_ids']) for c in df]\n",
    "seq_lengths_tokenizer2_q = [len(tokenizer2(c)['input_ids']) for c in df]\n",
    "\n",
    "# For train questions\n",
    "print(f'***For bert-base-multilingual-cased tokenizer***')\n",
    "show_dist(seq_lengths_tokenizer1_q)\n",
    "\n",
    "print('_'*100)\n",
    "\n",
    "print(f'***For klue/bert-base tokenizer***')\n",
    "show_dist(seq_lengths_tokenizer2_q)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
